import functools
import glob
import logging
import multiprocessing
import os
import pickle
import shutil
import sys
from argparse import ArgumentParser
from typing import List

import h5pickle
import numpy as np
from tqdm import tqdm

logging.basicConfig(
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
    level=os.environ.get("LOGLEVEL", "INFO").upper(),
    stream=sys.stdout,
)
logger = logging.getLogger(__name__)

global_event_to_vec = None


def init_worker(event_to_vec):
    global global_event_to_vec
    global_event_to_vec = event_to_vec
    logger.info(
        f"Process {multiprocessing.current_process().name} initialized with data"
    )


def get_parser():
    parser = ArgumentParser()
    parser.add_argument(
        "root",
        help="path to the **processed** MEDS dataset containing subdirectories for each split. "
        "it will try to scan all **/*.h5 files existed in this directory except for "
        "unique_events_*.h5 and process them.",
    )
    parser.add_argument(
        "--map_dir",
        help="path to the directory containing **`event_to_vec.pkl`**, which is generated by "
        "running the model with `--encode_events=True` and `--encode_only=True`",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="outputs",
        help="directory to save the processed outputs.",
    )
    parser.add_argument(
        "--workers",
        metavar="N",
        default=1,
        type=int,
        help="number of parallel workers.",
    )

    return parser


def main(args):
    filelist = glob.glob(os.path.join(args.root, "**/*.h5"))
    files = [h5pickle.File(fname) for fname in filelist if "unique_events" not in fname]
    logger.info("Reading mapping dictionary for event vectors...")
    with open(os.path.join(args.map_dir, "event_to_vec.pkl"), "rb") as f:
        event_to_vec = pickle.load(f)

    subdirs = [
        os.path.relpath(os.path.dirname(p), os.path.abspath(args.root))
        for p in filelist
        if "unique_events" not in p
    ]
    output_dirs = [os.path.join(args.output_dir, subdir) for subdir in subdirs]
    for subdir in np.unique(subdirs):
        if os.path.exists(os.path.join(args.output_dir, subdir)):
            shutil.rmtree(os.path.join(args.output_dir, subdir))
        os.makedirs(os.path.join(args.output_dir, subdir))

    logger.info("Mapping events to their representation vectors...")
    if args.workers <= 1:
        _map_events_to_vec(event_to_vec, output_dirs, files)
    else:
        n = args.workers
        files_chunks = [files[i::n] for i in range(n)]
        output_dir_chunks = [output_dirs[i::n] for i in range(n)]

        pool = multiprocessing.Pool(
            processes=args.workers, initializer=init_worker, initargs=(event_to_vec,)
        )
        pool.starmap(_map_events_to_vec, zip(output_dir_chunks, files_chunks))
        pool.close()
        pool.join()


def _map_events_to_vec(output_dirs: List[str], files: List[h5pickle.File]):
    assert len(files) == len(output_dirs)
    for f, output_dir in zip(files, output_dirs):
        output_name = os.path.basename(f.filename).split(".h5")[0] + "_encoded.h5"

        with h5pickle.File(os.path.join(output_dir, output_name), "w") as output_f:
            output_f.create_group("ehr")
            for sbj_id in tqdm(f["ehr"], total=len(f["ehr"]), desc=output_name):
                input_ids = f["ehr"][sbj_id]["hi"][:, 0]  # S, 128
                event_tuples = [tuple(event[event != 0]) for event in input_ids]
                event_vectors = np.array(
                    [global_event_to_vec[event] for event in event_tuples]
                )
                output_f["ehr"].create_group(sbj_id)
                output_f["ehr"][sbj_id].create_dataset(
                    "encoded",
                    data=event_vectors,
                    dtype="i2",
                    compression="lzf",
                    shuffle=True,
                    chunks=event_vectors.shape,
                )
                output_f["ehr"][sbj_id].create_dataset(
                    "time",
                    data=f["ehr"][sbj_id]["time"][()],
                )
                output_f["ehr"][sbj_id].create_dataset(
                    "label", data=f["ehr"][sbj_id]["label"][()]
                )


if __name__ == "__main__":
    parser = get_parser()
    args = parser.parse_args()
    main(args)
